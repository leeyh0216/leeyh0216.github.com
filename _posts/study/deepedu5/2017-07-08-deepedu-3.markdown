---
layout: post
title:  "[DeepEdu5] 3장 신경망"
date:   2017-07-08 21:00:00 +0900
author: leeyh0216
categories: deeplearning study deepedu5
---

> 이 문서는 모두의 연구소 DeepLearning Edu 5기 7월 8일 스터디 진행을 위해 작성된 문서입니다.

# 신경망

## 배경지식

### 퍼셉트론의 한계

앞의 2장에서 퍼셉트론의 **가중치**와 **편향**을 우리가 정해주었다.
즉, 데이터를 나누는 **기준**을 우리가 정해주었다는 뜻이다.

데이터를 모두 알고, 이에 따른 가중치와 편향을 우리가 조절해준 상태에서 퍼셉트론은 훌륭한 분류기 역할을 수행한다.

하지만 우리가 정해준 **기준**에 맞지 않는 데이터가 들어오는 순간, 퍼셉트론은 바보기계가 되고 만다. 새로운 데이터가 들어올 때는 항상 퍼셉트론의 기준인 가중치와 편향을 우리가 조정해 주어야 한다.

### 퍼셉트론의 한계를 뛰어넘은 신경망

신경망은 데이터를 나누는 **기준**을 데이터를 보면서 **학습**한다.
즉, 우리는 신경망에 데이터를 넣어서 신경망이 데이터에 적합한 가중치와 편향을 만들 수 있도록 도와주면 된다.

## 주요 내용

### 신경망의 구조

<img src="https://user-images.githubusercontent.com/7621901/28827121-81a16f38-7707-11e7-82f1-83df7d4a5f2d.png"/>

신경망은 위와 같이 입력층, 은닉층, 출력층으로 구성되어 있다. 입력층, 출력층은 1개이고 은닉층은 1개 이상으로 구성될 수 있다.

원으로 표현된 구조는 **노드** 또는 **뉴런** 이라고 부르며, 뉴런들을 잇는 직선들을 **가중치** 라고 부른다.

### Foward Propagation

위 신경망의 구조에서 각 층 사이를 확대해보자.

<img src="https://user-images.githubusercontent.com/7621901/28827738-763c54da-7709-11e7-90bf-83d55375a4cb.png"/>

입력 노드 X1,X2가 출력 노드 Y와 가중치 w1,w2로 연결되어 있다.
출력노드 Y는 퍼셉트론과 동일하게 다음과 같이 정의할 수 있다.

Y = X1\*w1 + X2\*w2

여기에 편향(bias, b)를 더하여 노드(뉴런)의 활성을 나타낼 수 있다.

<img src="https://user-images.githubusercontent.com/7621901/28828035-758542c6-770a-11e7-8ade-4392385f3114.png"/>

위 그림에서 각 가중치는 입력 노드의 값이 출력 노드의 값에 미치는 영향력, 편향은 출력 노드가 활성화 되는 기준이라고 생각하면 된다. 즉, 가중치의 값이 클수록 가중치에 연결된 입력 노드의 출력에 대한 영향력이 커지고, 편향이 작을 수록 출력 노드의 값이 1이 될 가능성이 커진다.

위의 식에서, Y값은 입력 값이 0보다 작거나 같을 때 0, 클 때는 1으로 결정된다. 이렇게 0, 1로 입력 값을 나누어 주는 함수를 '계단함수'라고 한다. 

출력 노드의 값(Y)가 1일 때 **활성 상태** 라고 하고, 0일 때 **비활성 상태**라고 한다.
이렇게 값이 활성화 되었는지, 비활성화 되었는지 확인 시켜주는 함수를 **활성 함수**라고 한다.
계단 함수는 활성 함수의 한 종류이다.

이렇게 입력 값과 가중치를 서로 곱한 후 편향을 더해 나온 값을 활성 함수에 통과시키는 과정을 Foward Propagation이라고 한다.

### 비선형 함수

앞의 Foward Propagation 과정에서 활성 함수(계단 함수)를 통해 Y값의 활성 상태를 확인하였다.
활성 함수는 비선형 함수를 사용하여야 한다.

'함수'는 입력을 넣었을 때 일련의 과정을 거쳐 출력을 내놓는 작용이다.
아래와 같이 일반적인 1차 함수를 우리는 '선형함수' 라고 한다.

y = ax

그렇다면 비선형 함수는 무엇일까? 위키피디아의 비선형의 정의는 다음과 같다.

> 계, 변환 등이 비선형이라는 것은 그 구성요소의 합이나 곱 등 선형 결합으로 설명할 수 없다는 것을 뜻한다.

즉, 단순히 특정한 값을 곱하거나 더해서 결과가 나오는 것이 아닌 제곱, 나눗셈 등의 여러 변환을 거친 것을 의미한다.
아래는 비선형 함수의 예이다.

y = x^2

y = 1/x

그렇다면 왜 Forward Propagation에서는 비선형 함수를 이용하는 것일까? 신경망은 입력 값을 어떤 함수에 통과시켜 공간에 흩뿌린 뒤, 이 공간을 나누는 과정이다. 여기서 공간을 나누는 선이 곧 신경망이 만들어내는 공식인데, 이 공식이 선형 함수라면, 하나의 곧은 직선으로 공간 하나를 두개로 나누는 일밖에 할 수 없을 것이다.

하지만 만약 이 공식이 비선형 함수라면, 우리는 공간을 어떠한 형태로든 나누어 낼 수 있다.
즉, 입력 값이 단순히 둘로 나뉘는 경우가 아니라면 비선형 함수를 사용해야 하고, Foward Propagation에서 비선형 함수를 사용하는 이유인 것이다.
