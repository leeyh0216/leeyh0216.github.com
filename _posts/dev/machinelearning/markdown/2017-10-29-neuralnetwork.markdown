---
layout: post
title:  "[ML] 신경망(Neural Network)"
date:   2017-10-29 17:47:00 +0900
author: leeyh0216
categories: machinelearning neuralnetwork
---

# 신경망 기초

## 인공 신경망의 구조

인공 신경망은 아래와 같이 '입력층','은닉층','출력층'으로 구성되어 있으며, 각 층은 '뉴런(또는 노드)'으로 이루어져 있고, 층 간의 뉴런은 '가중치'로 연결되어 있다.

![신경망(출처 : 위키백과, https://ko.wikipedia.org/wiki/%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D)](https://upload.wikimedia.org/wikipedia/commons/thumb/4/46/Colored_neural_network.svg/300px-Colored_neural_network.svg.png)

요즘 나오는 각종 머신러닝/딥러닝 책에서는 구조를 설명한 이후에 바로 계산식을 설명하지만, 우리 뇌의 구조를 이해하면 이러한 계산을 왜 수행하는지 알 수 있다.

![뇌의 구조(출처 : http://cinema4dr12.tistory.com/1007)]({{site.url}}/assets/nn_2.png)

우리 뇌의 구조는 아래와 같다. 우리 몸을 구성하는 신경에서 어떤 외부자극으로 인해 전기신호를 발생시키면 아래 그림의 Dendrite(수상돌기)로 입력되고, 입력에 따라 중요도(가중치)를 부여하여 출력신호를 만들게 된다. 이 출력신호가 특정 임계치를 넘어서게 되면 다음 뉴런으로 신호가 전이되고, 마지막 뉴런까지 도달하여 특정 출력을 만들어 우리가 반응하게 되는 것이다.

아래는 인공 신경망의 일부를 확대한 그림이다.

![뉴런 연산]({{site.url}}/assets/nn_1.png)

X1,X2,X3는 수상돌기 역할을 수행하는 입력 뉴런이다. 입력 뉴런은 첫 입력이나 이전 층의 뉴런에서 넘어온 출력값을 입력으로 받아들인다. 

X1,X2,X3와 Y를 연결하는 직선을 가중치라 부른다. 가중치는 각 입력 값의 중요도를 의미하는 값으로써 가중치가 클수록 해당 가중치와 연결된 입력 뉴런이 중요하다는 것을 의미한다. 

b 값은 편향(bias)라고 부른다. 편향은 뇌 구조에서 '임계치'를 의미하며 입력값(X1,X2,X3)과 가중치(W1,W2,W3)를 곱한 값에 더해진 후 '활성화 함수'를 거쳐 최종 출력값 Y를 만들게 된다. '활성화 함수'는 특정 임계값을 기준으로 출력값을 바꾸어 주는 함수로써 활성화 함수 중의 하나인 '계단함수'는 0보다 큰 값의 경우 1, 0보다 작거나 같은 값의 경우 0을 출력하게 된다. 즉, b값이 클수록 출력이 쉽게 활성화되고 b값이 작을수록 활성화되기 어렵다.

이러한 계산과정을 거쳐 출력층을 구성하는 뉴런들의 값을 계산하게 되고, 출력층의 뉴런 중 가장 큰 값(계단함수로 구현한 경우 1인 뉴런)이 정답이 되도록 가중치와 편향을 학습하여 학습에 이용하지 않은 입력이라도 정답을 맞출 수 있게 하는 것이 인공신경망의 학습 과정이다.

## 활성화 함수

활성화 함수의 입력이 특정 임계치(편향, bias)를 넘게 되면 이를 '활성화'되었다고 한다. 인공신경망에서 활성화 함수로는 '비선형 함수'를 사용한다. 비선형 함수는 하나의 직선으로 그래프를 그릴 수 없는 함수를 의미하고 대표적으로 '계단함수','시그모이드 함수','Relu' 등이 존재한다.

활성화 함수가 비선형함수여야하는 이유는 출력값을 계산하는 수식을 그래프로 나타내보면 알 수 있다. 만일 활성화 함수가 선형함수인 경우, 여러개의 층으로 이루어졌더라도 출력값이 입력값으로 이루어진 1차식이 된다. 1차식 그래프는 공간을 단순히 2개로 밖에 나눌 수 없으므로 결과가 3개 이상의 값으로 구분되어야 하는 경우 표현이 불가능하다. 하지만 비선형 함수를 활성화 함수로 사용하는 인공신경망의 경우 3개 이상의 값으로 공간을 나눌 수 있다. 따라서 활성화 함수가 비선형 함수여야만 신경망의 층을 깊게하는 의미가 있다.

### 계단함수

계단 함수는 입력값이 특정 임계치를 넘느냐 아니냐에 따라 다른 값을 출력하는 함수이다. 다음 수식은 입력값이 0을 넘는 경우 1, 0을 넘지 못하는 경우 0을 출력하는 계단함수이다.

![계단 함수]({{site.url}}/assets/step_function.png)

위 함수를 python으로 구현하면 아래와 같다.

{% include dev/machinelearning/notebook/step_function.html %}

### 시그모이드 함수

시그모이드 함수는 Relu 함수가 나오기 전까지(왜 Relu가 나오기 전까지 많이 사용했는지는 다음 글에서 설명할 예정) 인공신경망에서 가장 많이 사용했던 활성화 함수이다. 시그모이드 함수는 아래와 같은 수식으로 표현된다.

![시그모이드 함수]({{site.url}}/assets/sigmoid_function.png)

시그모이드 함수는 작은 값일수록 0에 가깝게, 큰 값일수록 1에 가깝게 출력을 내보낸다. python으로 구현하면 아래와 같다.

{% include dev/machinelearning/notebook/sigmoid_function.html %}

## Foward Propagation

입력으로 들어온 값을 은닉층을 거쳐 출력층까지 계산하는 과정을 Foward Propagation이라고 한다.
입력층 1개, 은닉층 1개, 출력층 1개로 이루어진 인공신경망을 python 코드로 구현해볼 것이다.
입력층, 은닉층, 출력층 각각 3,5,2개의 뉴런으로 구성되어 있는 인공신경망의 예시이다.

{% include dev/machinelearning/notebook/forward_propagation.html %}

## 신경망 학습

신경망에서 입력값을 제외한 가중치와 편향은 초기에 랜덤하게 설정된다. 그리고 입력값을 넣어 해당 입력값의 정답이 되는 출력 뉴런의 값을 가장 크게 만드는 학습 과정을 통해 가중치와 편향을 조정한다.


작성중...
