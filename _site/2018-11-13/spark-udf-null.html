<!DOCTYPE html>
<html lang="en">

  <head>
  
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Spark UDF와 DataSet에서의 NULL 처리 | DevLog</title>
<meta name="generator" content="Jekyll v3.8.6" />
<meta property="og:title" content="Spark UDF와 DataSet에서의 NULL 처리" />
<meta name="author" content="leeyh0216" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="개요 Spark SQL에서는 UDF(User Defined Function)를 만들 수 있는 기능을 제공한다. SQL만으로 처리가 힘들거나 코드가 지저분해지는 상황이 발생했을 때 유용하게 사용할 수 있다. 다만 NULL 처리에 관해서는 매우 신경을 써 줘야하는데, 오늘 1시간 넘게 UDF 구현 시 NULL 관련 오류를 접했던 삽질을 정리한다. UDF에서의 NULL 처리 String Type String 타입의 경우 애초에 Reference 타입이기 때문에, UDF에서의 NULL 처리가 간결하다. 주어진 문자열을 시작 위치부터 2만큼 잘라내는 UDF를 만들어보자. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;SELECT &quot;hello&quot; AS a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(1,false) +------+ |result| +------+ |he | +------+ 정상적으로 처리되는 것을 확인할 수 있다. 이제 NULL 값도 추가하여 테스트를 진행해보자. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 Seq(&quot;hello&quot;, null).toDF(&quot;a&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(2,false) org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (string) =&gt; string) at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1058) at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:139) ...생략 Caused by: java.lang.NullPointerException at $anonfun$1.apply(&lt;console&gt;:23) at $anonfun$1.apply(&lt;console&gt;:23) ...생략 위와 같이 NullPointerException이 발생하는 것을 확인할 수 있다. 이 경우는 UDF에서 인자 s가 NULL인 경우에 대한 예외처리만 해주면 된다. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; if(s == null) &quot;&quot; else s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 Seq(&quot;hello&quot;, null).toDF(&quot;a&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(2,false) /**************** 실행 결과 +------+ |UDF(a)| +------+ |he | | | +------+ *****************/ 위와 같이 일반적인 NULL 처리 방식으로 쉽게 구현이 가능한 것을 확인할 수 있다. Int, Long 등의 숫자 Primitive Type 오늘 삽질의 원인이 되었던 Integer, Long 타입이다. 처음에는 처리하려던 필드가 null이 발생할 수 있는 필드인지 몰랐기 때문에, UDF 인자를 모두 Primitive Type인 Int와 Long 등으로 정의했다. 실제 회사 코드를 가져올 수는 없으니, Int 타입의 값을 받아 1 증가시켜 반환하는 incrUDF 라는 UDF를 정의한 후 테스트해보도록 하자. //UDF 생성 및 등록 val incrUDF = udf { a: Int =&gt; a+1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select 1 as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |2 | +------+ *****************/ 정상적으로 1 값에 1을 더해 2를 반환하여 결과가 2로 나타난 것을 볼 수 있다. 그렇다면 a 필드에 null 값을 넣어보면 어떨까? //UDF 생성 및 등록 val incrUDF = udf { a: Int =&gt; a+1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select cast(null as int) as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |null | +------+ *****************/ null이 반환된 것을 볼 수 있다. 사실 위 코드에서 의도했던 결과는 1이었다. 이유는 아래와 같은 코드를 작성해보면 알 수 있다. scala&gt; val tmp = null.asInstanceOf[Int] + 1 tmp: Int = 1 Scala의 null을 asInstanceOf 메소드를 이용하여 캐스팅해보면 Int의 기본 값인 0이 되는 것을 확인할 수 있다. 당연히 Spark이 Scala 위에서 구현되었기 때문에 UDF도 언어적인 측면을 따라갈 것이라 생각했지만, SQL 내에서 실행되는 함수이기 때문에 NULL 처리 또한 SQL을 따라가고 있었다. 이러한 문제를 피하기 위해서는 Primitive Type이 아닌 Object Type을 사용하면 된다. 위의 코드를 아래와 같이 변경하여 테스트해보면 정상적으로 동작하는 것을 확인할 수 있다. //UDF 생성 및 등록 val incrUDF = udf { a: java.lang.Integer =&gt; if(a == null) 1 else a.intValue() + 1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select cast(null as int) as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |1 | +------+ *****************/ 의도한 대로 동작하는 것을 확인할 수 있다. DataSet 사용 시의 null 처리 만일 Dataframe을 Case Class에 매핑시켜 Dataset으로 만들었을 때는 각 타입들이 어떻게 동작할까? 아래와 같은 코드를 이용하여 테스트 해 보았다. case class A(a: Int, b: Long, c: String) spark.sql(&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;).as[A].map(r =&gt; A(r.a + 1, r.b + 1, r.c + &quot; is string&quot;)) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589) ... 생략 Caused by: java.lang.NullPointerException: Null value appeared in non-nullable field: - field (class: &quot;scala.Int&quot;, name: &quot;a&quot;) - root class: &quot;A&quot; If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int). ... 생략 위와 같이 오류가 발생하는 것을 확인할 수 있고, non-nullable 필드에 null이 발생하였으니, 해당 필드를 Option으로 감싸주라는 제안이 나온다. 그렇다면 A 클래스의 Primitive Type인 a(Int)와 b(Long)을 Option으로 감싸서 처리해보자. case class A(a: Int, b: Long, c: String) spark.sql(&quot;&quot;&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;&quot;&quot;).as[A].map(r =&gt; A(Some(r.a.getOrElse(0) + 1), Some((r.b.getOrElse(0L) + 1L)), r.c + &quot; is string&quot;)).toDF().show(1,false) +---+---+--------------+ |a |b |c | +---+---+--------------+ |1 |1 |null is string| +---+---+--------------+ 위와 같이 a, b가 null일 때는 정상적으로 1이 출력되고 c의 경우 null이 문자열처럼 취급되어 null is string이 출력되는 것을 확인할 수 있다. String의 경우 asInstanceOf[String]이 붙어 처리되는 듯 하다. String도 Option으로 처리해보면 어떨까? case class A(a: Int, b: Long, c: String) spark.sql(&quot;&quot;&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;&quot;&quot;).as[A].map(r =&gt; A(Some(r.a.getOrElse(0) + 1), Some((r.b.getOrElse(0L) + 1L)), Some(r.c.getOrElse(&quot;&quot;) + &quot; is string&quot;))).toDF().show(1,false) +---+---+----------+ |a |b |c | +---+---+----------+ |1 |1 | is string| +---+---+----------+ 위와 같이 String 또한 null일 경우 None으로 처리되는 것을 확인할 수 있다. 결론 따라서 Spark SQL 사용 시 null 값에 대한 확실한 처리를 위해서는 UDF 작성 시 Primitive Type이 아닌 Object Type 사용 Case Class 사용 시 Option 사용 을 유의해주어야 한다." />
<meta property="og:description" content="개요 Spark SQL에서는 UDF(User Defined Function)를 만들 수 있는 기능을 제공한다. SQL만으로 처리가 힘들거나 코드가 지저분해지는 상황이 발생했을 때 유용하게 사용할 수 있다. 다만 NULL 처리에 관해서는 매우 신경을 써 줘야하는데, 오늘 1시간 넘게 UDF 구현 시 NULL 관련 오류를 접했던 삽질을 정리한다. UDF에서의 NULL 처리 String Type String 타입의 경우 애초에 Reference 타입이기 때문에, UDF에서의 NULL 처리가 간결하다. 주어진 문자열을 시작 위치부터 2만큼 잘라내는 UDF를 만들어보자. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;SELECT &quot;hello&quot; AS a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(1,false) +------+ |result| +------+ |he | +------+ 정상적으로 처리되는 것을 확인할 수 있다. 이제 NULL 값도 추가하여 테스트를 진행해보자. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 Seq(&quot;hello&quot;, null).toDF(&quot;a&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(2,false) org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (string) =&gt; string) at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1058) at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:139) ...생략 Caused by: java.lang.NullPointerException at $anonfun$1.apply(&lt;console&gt;:23) at $anonfun$1.apply(&lt;console&gt;:23) ...생략 위와 같이 NullPointerException이 발생하는 것을 확인할 수 있다. 이 경우는 UDF에서 인자 s가 NULL인 경우에 대한 예외처리만 해주면 된다. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; if(s == null) &quot;&quot; else s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 Seq(&quot;hello&quot;, null).toDF(&quot;a&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(2,false) /**************** 실행 결과 +------+ |UDF(a)| +------+ |he | | | +------+ *****************/ 위와 같이 일반적인 NULL 처리 방식으로 쉽게 구현이 가능한 것을 확인할 수 있다. Int, Long 등의 숫자 Primitive Type 오늘 삽질의 원인이 되었던 Integer, Long 타입이다. 처음에는 처리하려던 필드가 null이 발생할 수 있는 필드인지 몰랐기 때문에, UDF 인자를 모두 Primitive Type인 Int와 Long 등으로 정의했다. 실제 회사 코드를 가져올 수는 없으니, Int 타입의 값을 받아 1 증가시켜 반환하는 incrUDF 라는 UDF를 정의한 후 테스트해보도록 하자. //UDF 생성 및 등록 val incrUDF = udf { a: Int =&gt; a+1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select 1 as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |2 | +------+ *****************/ 정상적으로 1 값에 1을 더해 2를 반환하여 결과가 2로 나타난 것을 볼 수 있다. 그렇다면 a 필드에 null 값을 넣어보면 어떨까? //UDF 생성 및 등록 val incrUDF = udf { a: Int =&gt; a+1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select cast(null as int) as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |null | +------+ *****************/ null이 반환된 것을 볼 수 있다. 사실 위 코드에서 의도했던 결과는 1이었다. 이유는 아래와 같은 코드를 작성해보면 알 수 있다. scala&gt; val tmp = null.asInstanceOf[Int] + 1 tmp: Int = 1 Scala의 null을 asInstanceOf 메소드를 이용하여 캐스팅해보면 Int의 기본 값인 0이 되는 것을 확인할 수 있다. 당연히 Spark이 Scala 위에서 구현되었기 때문에 UDF도 언어적인 측면을 따라갈 것이라 생각했지만, SQL 내에서 실행되는 함수이기 때문에 NULL 처리 또한 SQL을 따라가고 있었다. 이러한 문제를 피하기 위해서는 Primitive Type이 아닌 Object Type을 사용하면 된다. 위의 코드를 아래와 같이 변경하여 테스트해보면 정상적으로 동작하는 것을 확인할 수 있다. //UDF 생성 및 등록 val incrUDF = udf { a: java.lang.Integer =&gt; if(a == null) 1 else a.intValue() + 1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select cast(null as int) as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |1 | +------+ *****************/ 의도한 대로 동작하는 것을 확인할 수 있다. DataSet 사용 시의 null 처리 만일 Dataframe을 Case Class에 매핑시켜 Dataset으로 만들었을 때는 각 타입들이 어떻게 동작할까? 아래와 같은 코드를 이용하여 테스트 해 보았다. case class A(a: Int, b: Long, c: String) spark.sql(&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;).as[A].map(r =&gt; A(r.a + 1, r.b + 1, r.c + &quot; is string&quot;)) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589) ... 생략 Caused by: java.lang.NullPointerException: Null value appeared in non-nullable field: - field (class: &quot;scala.Int&quot;, name: &quot;a&quot;) - root class: &quot;A&quot; If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int). ... 생략 위와 같이 오류가 발생하는 것을 확인할 수 있고, non-nullable 필드에 null이 발생하였으니, 해당 필드를 Option으로 감싸주라는 제안이 나온다. 그렇다면 A 클래스의 Primitive Type인 a(Int)와 b(Long)을 Option으로 감싸서 처리해보자. case class A(a: Int, b: Long, c: String) spark.sql(&quot;&quot;&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;&quot;&quot;).as[A].map(r =&gt; A(Some(r.a.getOrElse(0) + 1), Some((r.b.getOrElse(0L) + 1L)), r.c + &quot; is string&quot;)).toDF().show(1,false) +---+---+--------------+ |a |b |c | +---+---+--------------+ |1 |1 |null is string| +---+---+--------------+ 위와 같이 a, b가 null일 때는 정상적으로 1이 출력되고 c의 경우 null이 문자열처럼 취급되어 null is string이 출력되는 것을 확인할 수 있다. String의 경우 asInstanceOf[String]이 붙어 처리되는 듯 하다. String도 Option으로 처리해보면 어떨까? case class A(a: Int, b: Long, c: String) spark.sql(&quot;&quot;&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;&quot;&quot;).as[A].map(r =&gt; A(Some(r.a.getOrElse(0) + 1), Some((r.b.getOrElse(0L) + 1L)), Some(r.c.getOrElse(&quot;&quot;) + &quot; is string&quot;))).toDF().show(1,false) +---+---+----------+ |a |b |c | +---+---+----------+ |1 |1 | is string| +---+---+----------+ 위와 같이 String 또한 null일 경우 None으로 처리되는 것을 확인할 수 있다. 결론 따라서 Spark SQL 사용 시 null 값에 대한 확실한 처리를 위해서는 UDF 작성 시 Primitive Type이 아닌 Object Type 사용 Case Class 사용 시 Option 사용 을 유의해주어야 한다." />
<link rel="canonical" href="https://leeyh0216.github.io/2018-11-13/spark-udf-null" />
<meta property="og:url" content="https://leeyh0216.github.io/2018-11-13/spark-udf-null" />
<meta property="og:site_name" content="DevLog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-11-13T10:00:00+09:00" />
<script type="application/ld+json">
{"author":{"@type":"Person","name":"leeyh0216"},"url":"https://leeyh0216.github.io/2018-11-13/spark-udf-null","mainEntityOfPage":{"@type":"WebPage","@id":"https://leeyh0216.github.io/2018-11-13/spark-udf-null"},"headline":"Spark UDF와 DataSet에서의 NULL 처리","dateModified":"2018-11-13T10:00:00+09:00","datePublished":"2018-11-13T10:00:00+09:00","description":"개요 Spark SQL에서는 UDF(User Defined Function)를 만들 수 있는 기능을 제공한다. SQL만으로 처리가 힘들거나 코드가 지저분해지는 상황이 발생했을 때 유용하게 사용할 수 있다. 다만 NULL 처리에 관해서는 매우 신경을 써 줘야하는데, 오늘 1시간 넘게 UDF 구현 시 NULL 관련 오류를 접했던 삽질을 정리한다. UDF에서의 NULL 처리 String Type String 타입의 경우 애초에 Reference 타입이기 때문에, UDF에서의 NULL 처리가 간결하다. 주어진 문자열을 시작 위치부터 2만큼 잘라내는 UDF를 만들어보자. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;SELECT &quot;hello&quot; AS a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(1,false) +------+ |result| +------+ |he | +------+ 정상적으로 처리되는 것을 확인할 수 있다. 이제 NULL 값도 추가하여 테스트를 진행해보자. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 Seq(&quot;hello&quot;, null).toDF(&quot;a&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(2,false) org.apache.spark.SparkException: Failed to execute user defined function($anonfun$1: (string) =&gt; string) at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:1058) at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:139) ...생략 Caused by: java.lang.NullPointerException at $anonfun$1.apply(&lt;console&gt;:23) at $anonfun$1.apply(&lt;console&gt;:23) ...생략 위와 같이 NullPointerException이 발생하는 것을 확인할 수 있다. 이 경우는 UDF에서 인자 s가 NULL인 경우에 대한 예외처리만 해주면 된다. //UDF 생성 및 등록 val subStrUDF = udf { s: String =&gt; if(s == null) &quot;&quot; else s.substring(0,2) } spark.sqlContext.udf.register(&quot;subStrUDF&quot;, subStrUDF) //UDF 테스트 Seq(&quot;hello&quot;, null).toDF(&quot;a&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;SELECT subStr(a) FROM tbl&quot;&quot;&quot;).show(2,false) /**************** 실행 결과 +------+ |UDF(a)| +------+ |he | | | +------+ *****************/ 위와 같이 일반적인 NULL 처리 방식으로 쉽게 구현이 가능한 것을 확인할 수 있다. Int, Long 등의 숫자 Primitive Type 오늘 삽질의 원인이 되었던 Integer, Long 타입이다. 처음에는 처리하려던 필드가 null이 발생할 수 있는 필드인지 몰랐기 때문에, UDF 인자를 모두 Primitive Type인 Int와 Long 등으로 정의했다. 실제 회사 코드를 가져올 수는 없으니, Int 타입의 값을 받아 1 증가시켜 반환하는 incrUDF 라는 UDF를 정의한 후 테스트해보도록 하자. //UDF 생성 및 등록 val incrUDF = udf { a: Int =&gt; a+1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select 1 as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |2 | +------+ *****************/ 정상적으로 1 값에 1을 더해 2를 반환하여 결과가 2로 나타난 것을 볼 수 있다. 그렇다면 a 필드에 null 값을 넣어보면 어떨까? //UDF 생성 및 등록 val incrUDF = udf { a: Int =&gt; a+1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select cast(null as int) as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |null | +------+ *****************/ null이 반환된 것을 볼 수 있다. 사실 위 코드에서 의도했던 결과는 1이었다. 이유는 아래와 같은 코드를 작성해보면 알 수 있다. scala&gt; val tmp = null.asInstanceOf[Int] + 1 tmp: Int = 1 Scala의 null을 asInstanceOf 메소드를 이용하여 캐스팅해보면 Int의 기본 값인 0이 되는 것을 확인할 수 있다. 당연히 Spark이 Scala 위에서 구현되었기 때문에 UDF도 언어적인 측면을 따라갈 것이라 생각했지만, SQL 내에서 실행되는 함수이기 때문에 NULL 처리 또한 SQL을 따라가고 있었다. 이러한 문제를 피하기 위해서는 Primitive Type이 아닌 Object Type을 사용하면 된다. 위의 코드를 아래와 같이 변경하여 테스트해보면 정상적으로 동작하는 것을 확인할 수 있다. //UDF 생성 및 등록 val incrUDF = udf { a: java.lang.Integer =&gt; if(a == null) 1 else a.intValue() + 1 } spark.sqlContext.udf.register(&quot;incrUDF&quot;, incrUDF) //UDF 테스트 spark.sql(&quot;&quot;&quot;select cast(null as int) as a&quot;&quot;&quot;).createOrReplaceTempView(&quot;tbl&quot;) spark.sql(&quot;&quot;&quot;select incrUDF(a) from tbl&quot;&quot;&quot;).show(1,false) /**************** +------+ |UDF(a)| +------+ |1 | +------+ *****************/ 의도한 대로 동작하는 것을 확인할 수 있다. DataSet 사용 시의 null 처리 만일 Dataframe을 Case Class에 매핑시켜 Dataset으로 만들었을 때는 각 타입들이 어떻게 동작할까? 아래와 같은 코드를 이용하여 테스트 해 보았다. case class A(a: Int, b: Long, c: String) spark.sql(&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;).as[A].map(r =&gt; A(r.a + 1, r.b + 1, r.c + &quot; is string&quot;)) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1602) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589) ... 생략 Caused by: java.lang.NullPointerException: Null value appeared in non-nullable field: - field (class: &quot;scala.Int&quot;, name: &quot;a&quot;) - root class: &quot;A&quot; If the schema is inferred from a Scala tuple/case class, or a Java bean, please try to use scala.Option[_] or other nullable types (e.g. java.lang.Integer instead of int/scala.Int). ... 생략 위와 같이 오류가 발생하는 것을 확인할 수 있고, non-nullable 필드에 null이 발생하였으니, 해당 필드를 Option으로 감싸주라는 제안이 나온다. 그렇다면 A 클래스의 Primitive Type인 a(Int)와 b(Long)을 Option으로 감싸서 처리해보자. case class A(a: Int, b: Long, c: String) spark.sql(&quot;&quot;&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;&quot;&quot;).as[A].map(r =&gt; A(Some(r.a.getOrElse(0) + 1), Some((r.b.getOrElse(0L) + 1L)), r.c + &quot; is string&quot;)).toDF().show(1,false) +---+---+--------------+ |a |b |c | +---+---+--------------+ |1 |1 |null is string| +---+---+--------------+ 위와 같이 a, b가 null일 때는 정상적으로 1이 출력되고 c의 경우 null이 문자열처럼 취급되어 null is string이 출력되는 것을 확인할 수 있다. String의 경우 asInstanceOf[String]이 붙어 처리되는 듯 하다. String도 Option으로 처리해보면 어떨까? case class A(a: Int, b: Long, c: String) spark.sql(&quot;&quot;&quot;select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c&quot;&quot;&quot;).as[A].map(r =&gt; A(Some(r.a.getOrElse(0) + 1), Some((r.b.getOrElse(0L) + 1L)), Some(r.c.getOrElse(&quot;&quot;) + &quot; is string&quot;))).toDF().show(1,false) +---+---+----------+ |a |b |c | +---+---+----------+ |1 |1 | is string| +---+---+----------+ 위와 같이 String 또한 null일 경우 None으로 처리되는 것을 확인할 수 있다. 결론 따라서 Spark SQL 사용 시 null 값에 대한 확실한 처리를 위해서는 UDF 작성 시 Primitive Type이 아닌 Object Type 사용 Case Class 사용 시 Option 사용 을 유의해주어야 한다.","@type":"BlogPosting","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->


  <!-- CSS -->
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Libre+Baskerville:400,400i,700">

  <!-- Favicon -->
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon-16x16.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/apple-touch-icon.png">

  <!-- RSS -->
  <link type="application/atom+xml" rel="alternate" href="https://leeyh0216.github.io/feed.xml" title="DevLog" />

  <!-- Google Analytics-->
  <script data-ad-client="ca-pub-8829030678254956" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
</head>


  <body>

    <nav class="nav">
  <div class="nav-container">
    <a href="/">
      <h2 class="nav-title">DevLog</h2>
    </a>
    <ul>
      <li><a href="/about">About</a></li>
      <li><a href="/">Posts</a></li>
      <li><a href="/tags">Tags</a></li>
    </ul>
  </div>
</nav>


    <main>
      <div class="post">
  <div class="post-info">
    <span>Written by</span>
    
        leeyh0216
    

    
      <br>
      <span>on&nbsp;</span><time datetime="2018-11-13 10:00:00 +0900">November 13, 2018</time>
    
    
    
      <br>
      <span>
      
        
        <a style="text-decoration: none;" href="/tag/apache-spark"><code class="highligher-rouge"><nobr>apache-spark</nobr></code>&nbsp;</a>
      
      </span>
    
  </span>
  </div>

  <h1 class="post-title">Spark UDF와 DataSet에서의 NULL 처리</h1>
  <div class="post-line"></div>

  <h1 id="개요">개요</h1>

<p>Spark SQL에서는 UDF(User Defined Function)를 만들 수 있는 기능을 제공한다.</p>

<p>SQL만으로 처리가 힘들거나 코드가 지저분해지는 상황이 발생했을 때 유용하게 사용할 수 있다.</p>

<p>다만 NULL 처리에 관해서는 매우 신경을 써 줘야하는데, 오늘 1시간 넘게 UDF 구현 시 NULL 관련 오류를 접했던 삽질을 정리한다.</p>

<h1 id="udf에서의-null-처리">UDF에서의 NULL 처리</h1>

<h2 id="string-type">String Type</h2>

<p>String 타입의 경우 애초에 Reference 타입이기 때문에, UDF에서의 NULL 처리가 간결하다.</p>

<p>주어진 문자열을 시작 위치부터 2만큼 잘라내는 UDF를 만들어보자.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//UDF 생성 및 등록
</span><span class="k">val</span> <span class="nv">subStrUDF</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">s</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span> <span class="nv">s</span><span class="o">.</span><span class="py">substring</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span><span class="mi">2</span><span class="o">)</span> <span class="o">}</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sqlContext</span><span class="o">.</span><span class="py">udf</span><span class="o">.</span><span class="py">register</span><span class="o">(</span><span class="s">"subStrUDF"</span><span class="o">,</span> <span class="n">subStrUDF</span><span class="o">)</span>

<span class="c1">//UDF 테스트
</span><span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""SELECT "hello" AS a"""</span><span class="o">).</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"tbl"</span><span class="o">)</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""SELECT subStr(a) FROM tbl"""</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="o">+------+</span>
<span class="o">|</span><span class="n">result</span><span class="o">|</span>
<span class="o">+------+</span>
<span class="o">|</span><span class="n">he</span>    <span class="o">|</span>
<span class="o">+------+</span></code></pre></figure>

<p>정상적으로 처리되는 것을 확인할 수 있다. 이제 NULL 값도 추가하여 테스트를 진행해보자.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//UDF 생성 및 등록
</span><span class="k">val</span> <span class="nv">subStrUDF</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">s</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span> <span class="nv">s</span><span class="o">.</span><span class="py">substring</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span><span class="mi">2</span><span class="o">)</span> <span class="o">}</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sqlContext</span><span class="o">.</span><span class="py">udf</span><span class="o">.</span><span class="py">register</span><span class="o">(</span><span class="s">"subStrUDF"</span><span class="o">,</span> <span class="n">subStrUDF</span><span class="o">)</span>

<span class="c1">//UDF 테스트
</span><span class="nc">Seq</span><span class="o">(</span><span class="s">"hello"</span><span class="o">,</span> <span class="kc">null</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"a"</span><span class="o">).</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"tbl"</span><span class="o">)</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""SELECT subStr(a) FROM tbl"""</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="nv">org</span><span class="o">.</span><span class="py">apache</span><span class="o">.</span><span class="py">spark</span><span class="o">.</span><span class="py">SparkException</span><span class="k">:</span> <span class="kt">Failed</span> <span class="kt">to</span> <span class="kt">execute</span> <span class="kt">user</span> <span class="kt">defined</span> <span class="kt">function</span><span class="o">(</span><span class="kt">$anonfun$1:</span> <span class="o">(</span><span class="kt">string</span><span class="o">)</span> <span class="o">=&gt;</span> <span class="kt">string</span><span class="o">)</span>
  <span class="n">at</span> <span class="nv">org</span><span class="o">.</span><span class="py">apache</span><span class="o">.</span><span class="py">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">.</span><span class="py">catalyst</span><span class="o">.</span><span class="py">expressions</span><span class="o">.</span><span class="py">ScalaUDF</span><span class="o">.</span><span class="py">eval</span><span class="o">(</span><span class="nv">ScalaUDF</span><span class="o">.</span><span class="py">scala</span><span class="k">:</span><span class="err">1058</span><span class="o">)</span>
  <span class="n">at</span> <span class="nv">org</span><span class="o">.</span><span class="py">apache</span><span class="o">.</span><span class="py">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">.</span><span class="py">catalyst</span><span class="o">.</span><span class="py">expressions</span><span class="o">.</span><span class="py">Alias</span><span class="o">.</span><span class="py">eval</span><span class="o">(</span><span class="nv">namedExpressions</span><span class="o">.</span><span class="py">scala</span><span class="k">:</span><span class="err">139</span><span class="o">)</span>
  <span class="o">...</span><span class="py">생략</span>
<span class="nc">Caused</span> <span class="n">by</span><span class="k">:</span> <span class="kt">java.lang.NullPointerException</span>
  <span class="n">at</span> <span class="nv">$anonfun$1</span><span class="o">.</span><span class="py">apply</span><span class="o">(&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">23</span><span class="o">)</span>
  <span class="n">at</span> <span class="nv">$anonfun$1</span><span class="o">.</span><span class="py">apply</span><span class="o">(&lt;</span><span class="n">console</span><span class="k">&gt;:</span><span class="mi">23</span><span class="o">)</span>
  <span class="o">...</span><span class="py">생략</span></code></pre></figure>

<p>위와 같이 NullPointerException이 발생하는 것을 확인할 수 있다.</p>

<p>이 경우는 UDF에서 인자 s가 NULL인 경우에 대한 예외처리만 해주면 된다.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//UDF 생성 및 등록
</span><span class="k">val</span> <span class="nv">subStrUDF</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">s</span><span class="k">:</span> <span class="kt">String</span> <span class="o">=&gt;</span> <span class="nf">if</span><span class="o">(</span><span class="n">s</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="s">""</span> <span class="k">else</span> <span class="nv">s</span><span class="o">.</span><span class="py">substring</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span><span class="mi">2</span><span class="o">)</span> <span class="o">}</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sqlContext</span><span class="o">.</span><span class="py">udf</span><span class="o">.</span><span class="py">register</span><span class="o">(</span><span class="s">"subStrUDF"</span><span class="o">,</span> <span class="n">subStrUDF</span><span class="o">)</span>

<span class="c1">//UDF 테스트
</span><span class="nc">Seq</span><span class="o">(</span><span class="s">"hello"</span><span class="o">,</span> <span class="kc">null</span><span class="o">).</span><span class="py">toDF</span><span class="o">(</span><span class="s">"a"</span><span class="o">).</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"tbl"</span><span class="o">)</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""SELECT subStr(a) FROM tbl"""</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">2</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="cm">/****************
    실행 결과
    +------+
    |UDF(a)|
    +------+
    |he    |
    |      |
    +------+
*****************/</span></code></pre></figure>

<p>위와 같이 일반적인 NULL 처리 방식으로 쉽게 구현이 가능한 것을 확인할 수 있다.</p>

<h2 id="int-long-등의-숫자-primitive-type">Int, Long 등의 숫자 Primitive Type</h2>

<p>오늘 삽질의 원인이 되었던 Integer, Long 타입이다.</p>

<p>처음에는 처리하려던 필드가 null이 발생할 수 있는 필드인지 몰랐기 때문에, UDF 인자를 모두 Primitive Type인 Int와 Long 등으로 정의했다.</p>

<p>실제 회사 코드를 가져올 수는 없으니, Int 타입의 값을 받아 1 증가시켜 반환하는 incrUDF 라는 UDF를 정의한 후 테스트해보도록 하자.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//UDF 생성 및 등록
</span><span class="k">val</span> <span class="nv">incrUDF</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">a</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=&gt;</span> <span class="n">a</span><span class="o">+</span><span class="mi">1</span> <span class="o">}</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sqlContext</span><span class="o">.</span><span class="py">udf</span><span class="o">.</span><span class="py">register</span><span class="o">(</span><span class="s">"incrUDF"</span><span class="o">,</span> <span class="n">incrUDF</span><span class="o">)</span>

<span class="c1">//UDF 테스트
</span><span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select 1 as a"""</span><span class="o">).</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"tbl"</span><span class="o">)</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select incrUDF(a) from tbl"""</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="cm">/****************
+------+
|UDF(a)|
+------+
|2     |
+------+
*****************/</span></code></pre></figure>

<p>정상적으로 1 값에 1을 더해 2를 반환하여 결과가 2로 나타난 것을 볼 수 있다.</p>

<p>그렇다면 a 필드에 null 값을 넣어보면 어떨까?</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//UDF 생성 및 등록
</span><span class="k">val</span> <span class="nv">incrUDF</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">a</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=&gt;</span> <span class="n">a</span><span class="o">+</span><span class="mi">1</span> <span class="o">}</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sqlContext</span><span class="o">.</span><span class="py">udf</span><span class="o">.</span><span class="py">register</span><span class="o">(</span><span class="s">"incrUDF"</span><span class="o">,</span> <span class="n">incrUDF</span><span class="o">)</span>

<span class="c1">//UDF 테스트
</span><span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select cast(null as int) as a"""</span><span class="o">).</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"tbl"</span><span class="o">)</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select incrUDF(a) from tbl"""</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="cm">/****************
+------+
|UDF(a)|
+------+
|null  |
+------+
*****************/</span></code></pre></figure>

<p>null이 반환된 것을 볼 수 있다. 사실 위 코드에서 의도했던 결과는 1이었다.</p>

<p>이유는 아래와 같은 코드를 작성해보면 알 수 있다.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="n">scala</span><span class="o">&gt;</span> <span class="k">val</span> <span class="nv">tmp</span> <span class="k">=</span> <span class="nv">null</span><span class="o">.</span><span class="py">asInstanceOf</span><span class="o">[</span><span class="kt">Int</span><span class="o">]</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">tmp</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">1</span></code></pre></figure>

<p>Scala의 null을 asInstanceOf 메소드를 이용하여 캐스팅해보면 Int의 기본 값인 0이 되는 것을 확인할 수 있다.</p>

<p>당연히 Spark이 Scala 위에서 구현되었기 때문에 UDF도 언어적인 측면을 따라갈 것이라 생각했지만, SQL 내에서 실행되는 함수이기 때문에 NULL 처리 또한 SQL을 따라가고 있었다.</p>

<p>이러한 문제를 피하기 위해서는 Primitive Type이 아닌 Object Type을 사용하면 된다.</p>

<p>위의 코드를 아래와 같이 변경하여 테스트해보면 정상적으로 동작하는 것을 확인할 수 있다.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="c1">//UDF 생성 및 등록
</span><span class="k">val</span> <span class="nv">incrUDF</span> <span class="k">=</span> <span class="n">udf</span> <span class="o">{</span> <span class="n">a</span><span class="k">:</span> <span class="kt">java.lang.Integer</span> <span class="o">=&gt;</span> <span class="nf">if</span><span class="o">(</span><span class="n">a</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="mi">1</span> <span class="k">else</span> <span class="nv">a</span><span class="o">.</span><span class="py">intValue</span><span class="o">()</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">}</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sqlContext</span><span class="o">.</span><span class="py">udf</span><span class="o">.</span><span class="py">register</span><span class="o">(</span><span class="s">"incrUDF"</span><span class="o">,</span> <span class="n">incrUDF</span><span class="o">)</span>

<span class="c1">//UDF 테스트
</span><span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select cast(null as int) as a"""</span><span class="o">).</span><span class="py">createOrReplaceTempView</span><span class="o">(</span><span class="s">"tbl"</span><span class="o">)</span>
<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select incrUDF(a) from tbl"""</span><span class="o">).</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="cm">/****************
+------+
|UDF(a)|
+------+
|1     |
+------+
*****************/</span></code></pre></figure>

<p>의도한 대로 동작하는 것을 확인할 수 있다.</p>

<h1 id="dataset-사용-시의-null-처리">DataSet 사용 시의 null 처리</h1>

<p>만일 Dataframe을 Case Class에 매핑시켜 Dataset으로 만들었을 때는 각 타입들이 어떻게 동작할까?</p>

<p>아래와 같은 코드를 이용하여 테스트 해 보았다.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">A</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">b</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">c</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span>

<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c"</span><span class="o">).</span><span class="py">as</span><span class="o">[</span><span class="kt">A</span><span class="o">].</span><span class="py">map</span><span class="o">(</span><span class="n">r</span> <span class="k">=&gt;</span> <span class="nf">A</span><span class="o">(</span><span class="nv">r</span><span class="o">.</span><span class="py">a</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="nv">r</span><span class="o">.</span><span class="py">b</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="nv">r</span><span class="o">.</span><span class="py">c</span> <span class="o">+</span> <span class="s">" is string"</span><span class="o">))</span>

<span class="nc">Driver</span> <span class="n">stacktrace</span><span class="k">:</span>
  <span class="kt">at</span> <span class="kt">org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages</span><span class="o">(</span><span class="kt">DAGScheduler.scala:</span><span class="err">1602</span><span class="o">)</span>
  <span class="kt">at</span> <span class="kt">org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply</span><span class="o">(</span><span class="kt">DAGScheduler.scala:</span><span class="err">1590</span><span class="o">)</span>
  <span class="kt">at</span> <span class="kt">org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply</span><span class="o">(</span><span class="kt">DAGScheduler.scala:</span><span class="err">1589</span><span class="o">)</span>
  <span class="kt">...</span> <span class="kt">생략</span>
<span class="nc">Caused</span> <span class="n">by</span><span class="k">:</span> <span class="kt">java.lang.NullPointerException:</span> <span class="kt">Null</span> <span class="kt">value</span> <span class="kt">appeared</span> <span class="kt">in</span> <span class="kt">non-nullable</span> <span class="kt">field:</span>
<span class="kt">-</span> <span class="kt">field</span> <span class="o">(</span><span class="kt">class:</span> <span class="err">"</span><span class="kt">scala.Int</span><span class="err">"</span><span class="o">,</span> <span class="kt">name:</span> <span class="err">"</span><span class="kt">a</span><span class="err">"</span><span class="o">)</span>
<span class="o">-</span> <span class="n">root</span> <span class="n">class</span><span class="k">:</span> <span class="err">"</span><span class="kt">A</span><span class="err">"</span>
<span class="kt">If</span> <span class="kt">the</span> <span class="kt">schema</span> <span class="kt">is</span> <span class="kt">inferred</span> <span class="kt">from</span> <span class="kt">a</span> <span class="kt">Scala</span> <span class="kt">tuple/case</span> <span class="kt">class</span><span class="o">,</span> <span class="n">or</span> <span class="n">a</span> <span class="nc">Java</span> <span class="n">bean</span><span class="o">,</span> <span class="n">please</span> <span class="k">try</span> <span class="n">to</span> <span class="n">use</span> <span class="nv">scala</span><span class="o">.</span><span class="py">Option</span><span class="o">[</span><span class="k">_</span><span class="o">]</span> <span class="n">or</span> <span class="n">other</span> <span class="n">nullable</span> <span class="nf">types</span> <span class="o">(</span><span class="nv">e</span><span class="o">.</span><span class="py">g</span><span class="o">.</span> <span class="nv">java</span><span class="o">.</span><span class="py">lang</span><span class="o">.</span><span class="py">Integer</span> <span class="n">instead</span> <span class="n">of</span> <span class="n">int</span><span class="o">/</span><span class="nv">scala</span><span class="o">.</span><span class="py">Int</span><span class="o">).</span>
<span class="o">...</span> <span class="n">생략</span></code></pre></figure>

<p>위와 같이 오류가 발생하는 것을 확인할 수 있고, non-nullable 필드에 null이 발생하였으니, 해당 필드를 Option으로 감싸주라는 제안이 나온다.</p>

<p>그렇다면 A 클래스의 Primitive Type인 a(Int)와 b(Long)을 Option으로 감싸서 처리해보자.</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">A</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">b</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">c</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span>

<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c"""</span><span class="o">).</span><span class="py">as</span><span class="o">[</span><span class="kt">A</span><span class="o">].</span><span class="py">map</span><span class="o">(</span><span class="n">r</span> <span class="k">=&gt;</span> <span class="nf">A</span><span class="o">(</span><span class="nc">Some</span><span class="o">(</span><span class="nv">r</span><span class="o">.</span><span class="py">a</span><span class="o">.</span><span class="py">getOrElse</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1</span><span class="o">),</span> <span class="nc">Some</span><span class="o">((</span><span class="nv">r</span><span class="o">.</span><span class="py">b</span><span class="o">.</span><span class="py">getOrElse</span><span class="o">(</span><span class="mi">0L</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1L</span><span class="o">)),</span> <span class="nv">r</span><span class="o">.</span><span class="py">c</span> <span class="o">+</span> <span class="s">" is string"</span><span class="o">)).</span><span class="py">toDF</span><span class="o">().</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>

<span class="o">+---+---+--------------+</span>
<span class="o">|</span><span class="n">a</span>  <span class="o">|</span><span class="n">b</span>  <span class="o">|</span><span class="n">c</span>             <span class="o">|</span>
<span class="o">+---+---+--------------+</span>
<span class="o">|</span><span class="mi">1</span>  <span class="o">|</span><span class="mi">1</span>  <span class="o">|</span><span class="kc">null</span> <span class="n">is</span> <span class="n">string</span><span class="o">|</span>
<span class="o">+---+---+--------------+</span></code></pre></figure>

<p>위와 같이 a, b가 null일 때는 정상적으로 1이 출력되고 c의 경우 null이 문자열처럼 취급되어 null is string이 출력되는 것을 확인할 수 있다.</p>

<p>String의 경우 asInstanceOf[String]이 붙어 처리되는 듯 하다.</p>

<p>String도 Option으로 처리해보면 어떨까?</p>

<figure class="highlight"><pre><code class="language-scala" data-lang="scala"><span class="k">case</span> <span class="k">class</span> <span class="nc">A</span><span class="o">(</span><span class="n">a</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">b</span><span class="k">:</span> <span class="kt">Long</span><span class="o">,</span> <span class="n">c</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span>

<span class="nv">spark</span><span class="o">.</span><span class="py">sql</span><span class="o">(</span><span class="s">"""select cast(null as int) as a, cast(null as long) as b, cast(null as string) as c"""</span><span class="o">).</span><span class="py">as</span><span class="o">[</span><span class="kt">A</span><span class="o">].</span><span class="py">map</span><span class="o">(</span><span class="n">r</span> <span class="k">=&gt;</span> <span class="nf">A</span><span class="o">(</span><span class="nc">Some</span><span class="o">(</span><span class="nv">r</span><span class="o">.</span><span class="py">a</span><span class="o">.</span><span class="py">getOrElse</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1</span><span class="o">),</span> <span class="nc">Some</span><span class="o">((</span><span class="nv">r</span><span class="o">.</span><span class="py">b</span><span class="o">.</span><span class="py">getOrElse</span><span class="o">(</span><span class="mi">0L</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1L</span><span class="o">)),</span> <span class="nc">Some</span><span class="o">(</span><span class="nv">r</span><span class="o">.</span><span class="py">c</span><span class="o">.</span><span class="py">getOrElse</span><span class="o">(</span><span class="s">""</span><span class="o">)</span> <span class="o">+</span> <span class="s">" is string"</span><span class="o">))).</span><span class="py">toDF</span><span class="o">().</span><span class="py">show</span><span class="o">(</span><span class="mi">1</span><span class="o">,</span><span class="kc">false</span><span class="o">)</span>
<span class="o">+---+---+----------+</span>
<span class="o">|</span><span class="n">a</span>  <span class="o">|</span><span class="n">b</span>  <span class="o">|</span><span class="n">c</span>         <span class="o">|</span>
<span class="o">+---+---+----------+</span>
<span class="o">|</span><span class="mi">1</span>  <span class="o">|</span><span class="mi">1</span>  <span class="o">|</span> <span class="n">is</span> <span class="n">string</span><span class="o">|</span>
<span class="o">+---+---+----------+</span></code></pre></figure>

<p>위와 같이 String 또한 null일 경우 None으로 처리되는 것을 확인할 수 있다.</p>

<h1 id="결론">결론</h1>

<p>따라서 Spark SQL 사용 시 null 값에 대한 확실한 처리를 위해서는</p>

<ul>
  <li>UDF 작성 시 Primitive Type이 아닌 Object Type 사용</li>
  <li>Case Class 사용 시 Option 사용</li>
</ul>

<p>을 유의해주어야 한다.</p>


</div>

<div class="pagination">
  
    <a href="/2018-11-19/elasticsearch-1" class="left arrow">&#8592;</a>
  
  
    <a href="/2018-11-10/spring-cloud-zuul" class="right arrow">&#8594;</a>
  

  <a href="#" class="top">Top</a>
</div>

    </main>

    <footer>
  <span>
    &copy; <time datetime="2019-11-05 21:11:02 +0900">2019</time> leeyh0216. Made with Jekyll using the <a href="https://github.com/chesterhow/tale/">Tale</a> theme.
  </span>
</footer>

  </body>
</html>
